{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Gentle Start\n",
    "\n",
    "* **Reading**: Chapter 2, Understanding Machine Learning by Shai Shalev-Shwartz and Shai Ben-David\n",
    "\n",
    "Basic notions in machine learning include the following elements.\n",
    "* The domain set is represented by $\\mathcal{X}$.\n",
    "* The label set is represented by $\\mathcal{Y}$ and, at the onset, we take $\\mathcal{Y} = \\{ 0, 1 \\}$.\n",
    "* The training data is denoted by $S = \\left\\{ (x_i, y_i) \\right\\}$, where each element in this set belongs to $\\mathcal{X} \\times \\mathcal{Y}$.\n",
    "\n",
    "\n",
    "## Goal\n",
    "\n",
    "* The learner wishes to create a prediction rule by $h : \\mathcal{X} \\mapsto \\mathcal{Y}$. This function is also called a predictor, a hypothesis, or a classifier.\n",
    "The rule should come from hypothesis class $\\mathcal{H}$.\n",
    "* It is sometime useful to denote a (correct) labeling function $f : \\mathcal{X} \\mapsto \\mathcal{Y}$ such that $y_i = f(x_i)$ for all $i$.\n",
    "\n",
    "\n",
    "### Design Objective\n",
    "\n",
    "* The last component needed is a measure of success. A typical such criterion can be derived from the probability of classification error:\n",
    "\\begin{equation*}\n",
    "L_{(\\mathcal{D}, f)} (h)\n",
    "\\doteq \\Pr_{x \\sim \\mathcal{D}} (h(x) \\neq f(x))\n",
    "= \\mathcal{D} \\left( \\{ x : h(x) \\neq f(x) \\} \\right) .\n",
    "\\end{equation*}\n",
    "In general, the nomenclature $L_{(\\mathcal{D}, f)} (\\cdot)$ refers to a loss function and $\\mathcal{D}$ is the probability distribution underlying $\\mathcal{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1.a\n",
    "\n",
    "Let $\\mathcal{D}$ be a multivariate normal distribution with location parameter $\\boldsymbol{\\mu} = \\mathbf{0}$ and covariance matrix $\\boldsymbol{\\Sigma} = \\mathbf{I}$.\n",
    "Moreover, assume that the labeling function $f : \\mathcal{X} \\mapsto \\mathbf{Y}$ is given by\n",
    "\\begin{equation*}\n",
    "f(\\mathbf{x}) = \\operatorname{sign} \\left( \\langle \\mathbf{x} | \\mathbf{v} \\rangle \\right)\n",
    "\\end{equation*}\n",
    "where $\\mathbf{v}$ is a fixed vector.\n",
    "A possible hypothesis class $\\mathcal{H}$ can be formed as follows.\n",
    "First, we identify a set of candidate vectors\n",
    "\\begin{equation*}\n",
    "\\mathbf{v}_k = (\\cos \\theta, \\sin \\theta)\n",
    "\\end{equation*}\n",
    "where $\\theta = \\frac{2 \\pi k}{N}$ and $k \\in [N]$.\n",
    "Then, we consider hypotheses of the form\n",
    "$h_k(\\mathbf{x}) = \\operatorname{sign} \\left( \\langle \\mathbf{x} | \\mathbf{v}_k \\rangle \\right)$.\n",
    "In this case, each vector partitions the plane into two half planes, and the boundary is a strainght line going through the origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Vector: [ 1.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkBmKHQAy839/27cFrinLIgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1180584a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Labeling function and, implicitly, hypothesis class\n",
    "N = 20\n",
    "theta = 2 * np.pi * np.random.randint(N) / N\n",
    "fvector = np.array([np.cos(theta), np.sin(theta)])\n",
    "perpvector = np.array([fvector[1], -fvector[0]])\n",
    "boundary = np.outer(perpvector, np.arange(-3,3.1,0.1))\n",
    "print('Label Vector: ' + str(fvector))\n",
    "\n",
    "# Distribution and training examples\n",
    "mu = 0\n",
    "sigma = 1\n",
    "X = np.random.normal(mu, sigma, (10,2))\n",
    "\n",
    "def labelfunction(x_i):\n",
    "    sign = np.sign(np.inner(x_i, fvector))\n",
    "    return sign\n",
    "\n",
    "XY_df = pd.DataFrame(X, columns=['pos1','pos2'])\n",
    "XY_df['Y'] = labelfunction(X)\n",
    "set1 = XY_df.loc[XY_df['Y'] > 0].as_matrix(['pos1','pos2'])\n",
    "set0 = XY_df.loc[XY_df['Y'] < 0].as_matrix(['pos1','pos2'])\n",
    "\n",
    "x, y = zip(*set1)\n",
    "plt.scatter(x, y, c='b')\n",
    "x, y = zip(*set0)\n",
    "plt.scatter(x, y, c='r')\n",
    "\n",
    "plt.plot(boundary[0,:],boundary[1,:], c='k')\n",
    "ax = plt.gca()\n",
    "ax.set_xlim([-4,4])\n",
    "ax.set_ylim([-4,4])\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical Risk Minimization (ERM)\n",
    "\n",
    "Given that only the training sample is available to the learner, it may be appealing to search for a solution that works well on $S$:\n",
    "\\begin{equation*}\n",
    "L_{S} (h) \\doteq \\frac{\\left| \\{ i \\in [m] : h(x_i) \\neq u_i \\} \\right|}{m} .\n",
    "\\end{equation*}\n",
    "The objective function is often referred to as the empirical error and empirical risk.\n",
    "Unfortunatly, ERM is prone to overfitting, a phenomenon where the hypothesis fits the training data well, but performs poorly in reality.\n",
    "\n",
    "\n",
    "## Inductive Bias\n",
    "\n",
    "A common strategy to prevent overfitting is to apply the ERM learning rule over a restricted hypothesis class $\\mathcal{H}$.\n",
    "The learner then uses the ERM rule to choose a predictor $h \\in \\mathcal{H}$, with the lowest possible error over $S$;\n",
    "\\begin{equation*}\n",
    "\\mathrm{ERM}_{\\mathcal{H}} (S) \\in \\operatorname{argmin}_{h \\in \\mathcal{H}} L_S (h) .\n",
    "\\end{equation*}\n",
    "Choosing a restricted hypothesis class can help protect against overfitting, but it may cause us a stronger inductive bias.\n",
    "\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "**Realizability Assumption:** There exists $h^{\\star} \\in \\mathcal{H}$ such that $L_{(\\mathcal{D}, f)} (h^{\\star}) = 0$.\n",
    "Note that this assumption implies that $L_S(h^{\\star}) = 0$ almost surely.\n",
    "\n",
    "**IID Assumption:** The points in the training set $S$ are drawn independently according to the distribution $\\mathcal{D}$. Consequently, $S \\sim \\mathcal{D}^m$ where $m$ is the cardinality of $S$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1.b\n",
    "\n",
    "Let $\\mathcal{H}$ be the hypothesis class given by\n",
    "\\begin{equation*}\n",
    "h_w(\\mathbf{x}) = \\operatorname{sign} \\left( \\langle \\mathbf{x} | \\mathbf{w} \\rangle \\right)\n",
    "\\end{equation*}\n",
    "where $\\mathbf{w}$ is a fixed vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Bounds\n",
    "\n",
    "There is some probability that the sampled training data $S$ is nonrepresentative of the underlying distribution $\\mathcal{D}$, in which case $L_{(\\mathcal{D}, f)} (h_S)$ may feature bad performance.\n",
    "The probability of getting a nonrepresentative sample is denoted by $\\delta$, and $(1 − \\delta)$ is call the confidence parameter of the prediction problem.\n",
    "\n",
    "Let $\\epsilon$ denote the accuracy parameter.\n",
    "We interpret the event $L_{(\\mathcal{D}, f)}(h_S) > \\epsilon$ as a failure of the learner, whereas the condition $L_{(\\mathcal{D}, f)}(h_S) \\leq \\epsilon$ is deemed an approximately correct predictor.\n",
    "We wish to bound\n",
    "\\begin{equation*}\n",
    "\\mathcal{D}^m \\left( \\left\\{ \\left. S \\right|_x : L_{(\\mathcal{D}, f)}(h_S) > \\epsilon \\right\\} \\right)\n",
    "\\end{equation*}\n",
    "Let $\\mathcal{H}_{\\mathrm{B}}$ be the set of bad hypothesis,\n",
    "\\begin{equation*}\n",
    "\\mathcal{H}_{\\mathrm{B}}\n",
    "= \\left\\{ h \\in \\mathcal{H} : L_{(\\mathcal{D}, f)}(h) > \\epsilon \\right\\} .\n",
    "\\end{equation*}\n",
    "Also, let\n",
    "\\begin{equation*}\n",
    "M = \\left\\{ \\left. S \\right|_x : \\exists h \\in \\mathcal{H}_{\\mathrm{B}}, L_S(h) = 0 \\right\\} .\n",
    "\\end{equation*}\n",
    "\n",
    "The event $L_{\\mathcal{D}, f}(h_S) > \\epsilon$ can only happen if for some $h \\in \\mathcal{H}_{\\mathrm{B}}$ we have $L_S (h) = 0$.\n",
    "In other words, this event will only happen if our sample is in the set of misleading samples, $M$,\n",
    "\\begin{equation*}\n",
    "\\left\\{ \\left. S \\right|_x :  L_{(\\mathcal{D}, f)}(h_S) > \\epsilon \\right\\} \\subseteq M.\n",
    "\\end{equation*}\n",
    "Then\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\mathcal{D}^m \\left( \\left\\{ \\left. S \\right|_x :  L_{(\\mathcal{D}, f)}(h_S) > \\epsilon \\right\\} \\right)\n",
    "&\\leq \\mathcal{D}^m (M) \\\\\n",
    "&\\leq \\sum_{h \\in \\mathcal{H}_{\\mathrm{B}}} \\mathcal{D}^m \\left( \\left\\{ \\left. S \\right|_x :  L_S(h) = 0 \\right\\} \\right) \\\\\n",
    "&= \\sum_{h \\in \\mathcal{H}_{\\mathrm{B}}} \\mathcal{D}^m \\left( \\left\\{ \\left. S \\right|_x : \\forall i, h(x_i) = f(x_i) \\right\\} \\right) \\\\\n",
    "&= \\sum_{h \\in \\mathcal{H}_{\\mathrm{B}}} \\prod_{i=1}^m \\mathcal{D} (h(x_i) = f(x_i)) .\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "For each individual sampling of an element of the training set we have\n",
    "\\begin{equation*}\n",
    "\\mathcal{D} (h(x_i) = f(x_i)) = 1 - L_{(\\mathcal{D}, f)}(h)\n",
    "\\leq 1 - \\epsilon .\n",
    "\\end{equation*}\n",
    "Combining these results, we get\n",
    "\\begin{equation*}\n",
    "\\mathcal{D}^m \\left( \\left\\{ \\left. S \\right|_x :  L_{(\\mathcal{D}, f)}(h_S) > \\epsilon \\right\\} \\right)\n",
    "\\leq \\left| \\mathcal{H}_{\\mathrm{B}} \\right| (1 - \\epsilon)^m\n",
    "\\leq \\left| \\mathcal{H} \\right| e^{- \\epsilon m} .\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probably Approximately Correct (PAC) Learning\n",
    "\n",
    "A finite hypothesis class $\\mathcal{H}$ is PAC learnable if there exists a function $m_{\\mathcal{H}} : (0,1)^2 \\mapsto \\mathbb{N}$ and a learning algorithm with the following property.\n",
    "For every $\\epsilon, \\delta \\in (0,1)$; for every $\\mathcal{D}$ over $\\mathcal{X}$ and every labeling function $f : \\mathcal{X} \\mapsto \\mathcal{Y}$ for which the realizability assumption holds;\n",
    "when running the learning algorithm on $m \\geq m_{\\mathcal{H}}(\\epsilon, \\delta)$ IID examples generated independently, the algorithm returns a hypothesis $h$ such that\n",
    "\\begin{equation*}\n",
    "\\mathcal{D}^m \\left( \\left\\{ \\left. S \\right|_x :  L_{(\\mathcal{D}, f)}(h_S) > \\epsilon \\right\\} \\right)\n",
    "\\leq \\delta .\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "In the present case, the bound above guarantees that, when\n",
    "\\begin{equation*}\n",
    "m \\geq \\frac{\\log \\left( \\left| \\mathcal{H} \\right| / \\delta \\right)}{\\epsilon} ,\n",
    "\\end{equation*}\n",
    "we have\n",
    "\\begin{equation*}\n",
    "\\mathcal{D}^m \\left( \\left\\{ \\left. S \\right|_x :  L_{(\\mathcal{D}, f)}(h_S) > \\epsilon \\right\\} \\right)\n",
    "\\leq \\delta .\n",
    "\\end{equation*}\n",
    "That is, for a sufficiently large $m$, the $\\mathrm{ERM}_{\\mathcal{H}}$ rule over a finite hypothesis class will be probably (with confidence $1 - \\delta$) approximately (up to an error of $\\epsilon$) correct.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
